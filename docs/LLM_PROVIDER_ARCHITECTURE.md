# LLM Provider Architecture

## üéØ M·ª•c ƒë√≠ch

Thi·∫øt k·∫ø abstraction layer ƒë·ªÉ d·ªÖ d√†ng switch gi·ªØa c√°c LLM providers m√† kh√¥ng c·∫ßn thay ƒë·ªïi business logic.

## üèóÔ∏è Architecture

### Strategy Pattern + Factory Pattern

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Business Logic                        ‚îÇ
‚îÇ              (views.py, services, etc.)                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚îÇ Uses
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              LLMProviderFactory                          ‚îÇ
‚îÇ         (Factory Pattern - creates providers)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚îÇ Creates
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              LLMProvider (Abstract)                      ‚îÇ
‚îÇ         (Strategy Pattern - interface)                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ              ‚îÇ              ‚îÇ
       ‚ñº              ‚ñº              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Ollama   ‚îÇ    ‚îÇ OpenAI   ‚îÇ   ‚îÇ DeepSeek ‚îÇ
‚îÇ Provider ‚îÇ    ‚îÇ Provider ‚îÇ   ‚îÇ Provider ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìÅ File Structure

```
app/services/
‚îú‚îÄ‚îÄ llm_providers/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py          # Register providers
‚îÇ   ‚îú‚îÄ‚îÄ base.py              # LLMProvider abstract class + Factory
‚îÇ   ‚îú‚îÄ‚îÄ ollama_provider.py   # Ollama implementation
‚îÇ   ‚îú‚îÄ‚îÄ openai_provider.py   # OpenAI implementation
‚îÇ   ‚îî‚îÄ‚îÄ deepseek_provider.py # DeepSeek implementation
‚îú‚îÄ‚îÄ llm_service.py           # High-level service (convenience functions)
‚îî‚îÄ‚îÄ ollama_client.py         # Existing Ollama client (wrapped by OllamaProvider)
```

## üîå Supported Providers

### 1. Ollama (Local)
- **Provider Name**: `ollama`
- **Models**: llama3.1, mistral, etc.
- **Features**: Chat, Embeddings, Local deployment
- **Config**: `OLLAMA_BASE_URL`, `OLLAMA_CHAT_MODEL`, `OLLAMA_EMBED_MODEL`

### 2. OpenAI
- **Provider Name**: `openai`
- **Models**: gpt-4, gpt-3.5-turbo, text-embedding-3-small, etc.
- **Features**: Chat, Embeddings
- **Config**: `OPENAI_API_KEY`

### 3. DeepSeek
- **Provider Name**: `deepseek`
- **Models**: deepseek-chat, deepseek-coder
- **Features**: Chat (no embeddings)
- **Config**: `DEEPSEEK_API_KEY`

## üíª Usage

### Basic Usage

```python
from app.services.llm_service import get_llm_provider

# Get default provider
provider = get_llm_provider()

# Get specific provider
provider = get_llm_provider('openai')

# Chat
response = provider.chat(
    messages=[{'role': 'user', 'content': 'Hello'}],
    model='gpt-3.5-turbo',
    stream=False
)

# Embeddings
embedding = provider.embed("text to embed")
```

### With ChatSession

```python
from app.services.llm_service import get_provider_for_session

# Get provider based on session.model_provider
provider = get_provider_for_session(chat_session)

# Use provider
response = provider.chat(messages, model=session.model_name)
```

### In Views

```python
# Old way (hardcoded Ollama)
from app.services.ollama_client import get_ollama_client
ollama = get_ollama_client()
response = ollama.chat(messages)

# New way (provider-agnostic)
from app.services.llm_service import get_provider_for_session
provider = get_provider_for_session(session)
response = provider.chat(messages, model=session.model_name)
```

## üîß Adding New Provider

### Step 1: Create Provider Class

```python
# app/services/llm_providers/anthropic_provider.py
from .base import LLMProvider

class AnthropicProvider(LLMProvider):
    @property
    def provider_name(self) -> str:
        return 'anthropic'
    
    def embed(self, prompt, model=None):
        # Implement embedding
        pass
    
    def chat(self, messages, model=None, stream=False, ...):
        # Implement chat
        pass
    
    def list_models(self):
        # Return available models
        pass
```

### Step 2: Register Provider

```python
# app/services/llm_providers/__init__.py
from .anthropic_provider import AnthropicProvider

LLMProviderFactory.register('anthropic', AnthropicProvider)
```

### Step 3: Update Model Choices

```python
# app/models.py
model_provider = models.CharField(
    choices=[
        ('ollama', 'Ollama'),
        ('openai', 'OpenAI'),
        ('anthropic', 'Anthropic'),  # Add new
        ('deepseek', 'DeepSeek'),
    ]
)
```

## ‚öôÔ∏è Configuration

### Settings

```python
# veritasai_django/settings.py

# Default provider
DEFAULT_LLM_PROVIDER = 'ollama'  # or 'openai', 'deepseek'

# Ollama
OLLAMA_BASE_URL = 'http://127.0.0.1:11434'
OLLAMA_CHAT_MODEL = 'llama3.1'
OLLAMA_EMBED_MODEL = 'nomic-embed-text'

# OpenAI
OPENAI_API_KEY = 'sk-...'

# DeepSeek
DEEPSEEK_API_KEY = 'sk-...'
```

## üîÑ Migration Path

### Current Code (Ollama-only)
```python
from app.services.ollama_client import get_ollama_client
ollama = get_ollama_client()
response = ollama.chat(messages)
```

### New Code (Provider-agnostic)
```python
from app.services.llm_service import get_llm_provider
provider = get_llm_provider('ollama')  # or 'openai', etc.
response = provider.chat(messages)
```

### Backward Compatibility
- `OllamaClient` v·∫´n ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng
- `OllamaProvider` wraps `OllamaClient`
- Existing code kh√¥ng b·ªã break

## üìä Benefits

1. **Flexibility**: D·ªÖ d√†ng switch providers
2. **Testability**: C√≥ th·ªÉ mock providers trong tests
3. **Extensibility**: D·ªÖ th√™m providers m·ªõi
4. **User Choice**: Users c√≥ th·ªÉ ch·ªçn provider per session
5. **Cost Optimization**: C√≥ th·ªÉ d√πng Ollama (free) cho dev, OpenAI cho production

## üéØ Future Enhancements

- [ ] Anthropic (Claude) provider
- [ ] Google Gemini provider
- [ ] Azure OpenAI provider
- [ ] Provider-specific features (function calling, etc.)
- [ ] Cost tracking per provider
- [ ] Automatic failover between providers

