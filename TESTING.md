# üß™ Testing Guide - Phase 1

## ‚úÖ ƒê√£ implement

1. ‚úÖ File upload endpoint (`POST /api/documents/upload/`)
2. ‚úÖ Celery task x·ª≠ l√Ω document (Django ORM)
3. ‚úÖ Token estimation service
4. ‚úÖ Vector search v·ªõi pgvector
5. ‚úÖ Chat v·ªõi RAG (vector search + LLM streaming)

## üöÄ Test Flow

### B∆∞·ªõc 1: Start Services

```bash
# Terminal 1: Django server
source venv/bin/activate
python manage.py runserver

# Terminal 2: Celery worker
source venv/bin/activate
celery -A app.celery_app worker --loglevel=info

# Terminal 3: Redis (n·∫øu ch∆∞a ch·∫°y)
redis-server
```

### B∆∞·ªõc 2: Upload File

```bash
# Test upload v·ªõi curl
curl -X POST http://localhost:8000/api/documents/upload/ \
  -F "file=@/path/to/your/document.pdf"

# Ho·∫∑c v·ªõi Python requests
python -c "
import requests
files = {'file': open('test.pdf', 'rb')}
response = requests.post('http://localhost:8000/api/documents/upload/', files=files)
print(response.json())
"
```

**Expected Response:**
```json
{
  "message": "File uploaded successfully",
  "document": {
    "id": 1,
    "name": "test.pdf",
    "status": "pending",
    ...
  }
}
```

### B∆∞·ªõc 3: Check Processing Status

```bash
# Check document status
curl http://localhost:8000/api/documents/1/

# Ho·∫∑c check trong admin panel
# http://localhost:8000/admin/app/document/
```

**Wait for status = "completed"** (check Celery worker logs)

### B∆∞·ªõc 4: Chat v·ªõi Document

```bash
# Test chat v·ªõi curl (Server-Sent Events)
curl -X POST http://localhost:8000/api/chat/stream/ \
  -H "Content-Type: application/json" \
  -d '{
    "document_id": 1,
    "messages": [
      {"role": "user", "content": "What is this document about?"}
    ]
  }'
```

**Expected Response (Streaming):**
```
data: {"content": "Based"}
data: {"content": " on"}
data: {"content": " the"}
...
```

### B∆∞·ªõc 5: Test v·ªõi Python Script

```python
# test_rag.py
import requests
import json

# 1. Upload file
with open('test.pdf', 'rb') as f:
    response = requests.post(
        'http://localhost:8000/api/documents/upload/',
        files={'file': f}
    )
    doc = response.json()['document']
    doc_id = doc['id']
    print(f"Uploaded document ID: {doc_id}")

# 2. Wait for processing (poll status)
import time
while True:
    response = requests.get(f'http://localhost:8000/api/documents/{doc_id}/')
    status = response.json()['status']
    print(f"Status: {status}")
    if status == 'completed':
        break
    elif status == 'failed':
        print("Processing failed!")
        break
    time.sleep(2)

# 3. Chat v·ªõi document
response = requests.post(
    'http://localhost:8000/api/chat/stream/',
    json={
        'document_id': doc_id,
        'messages': [
            {'role': 'user', 'content': 'What is this document about?'}
        ]
    },
    stream=True
)

print("\nChat Response:")
for line in response.iter_lines():
    if line:
        data = json.loads(line.decode('utf-8').replace('data: ', ''))
        if 'content' in data:
            print(data['content'], end='', flush=True)
        elif 'error' in data:
            print(f"\nError: {data['error']}")
            break
print("\n")
```

## üîç Check Logs

### Django Server Logs
```bash
# Xem logs trong terminal running Django server
```

### Celery Worker Logs
```bash
# Xem logs trong terminal running Celery worker
# S·∫Ω th·∫•y:
# - "Extracting text from document X"
# - "Chunking text for document X"
# - "Starting batch embedding generation"
# - "Document processing completed"
```

### Database Check
```bash
# Check documents
psql -d veritasai_python -c "SELECT id, name, status, num_chunks FROM documents;"

# Check chunks
psql -d veritasai_python -c "SELECT COUNT(*) FROM document_chunks WHERE document_id = 1;"

# Check chat messages
psql -d veritasai_python -c "SELECT role, content FROM chat_messages WHERE document_id = 1;"
```

## ‚ö†Ô∏è Troubleshooting

### 1. Celery kh√¥ng ch·∫°y
```bash
# Check Redis
redis-cli ping  # Should return PONG

# Check Celery worker
celery -A app.celery_app inspect active
```

### 2. Ollama kh√¥ng ch·∫°y
```bash
# Check Ollama
curl http://127.0.0.1:11434/api/tags

# Pull models n·∫øu ch∆∞a c√≥
ollama pull nomic-embed-text
ollama pull llama3.2
```

### 3. Vector search kh√¥ng ho·∫°t ƒë·ªông
```bash
# Check pgvector extension
psql -d veritasai_python -c "SELECT * FROM pg_extension WHERE extname = 'vector';"

# Check embeddings
psql -d veritasai_python -c "SELECT id, array_length(embedding::float[], 1) as dims FROM document_chunks LIMIT 1;"
```

### 4. File upload l·ªói
- Check storage directory permissions
- Check file size (< 10MB)
- Check file type (PDF, DOCX, TXT, MD only)

## üìù Test Checklist

- [ ] Upload PDF file
- [ ] Upload DOCX file
- [ ] Upload TXT file
- [ ] Check document processing (status: pending ‚Üí processing ‚Üí completed)
- [ ] Check chunks created in database
- [ ] Chat v·ªõi document (vector search ho·∫°t ƒë·ªông)
- [ ] Check chat messages saved
- [ ] Test v·ªõi multiple documents
- [ ] Test error handling (invalid file, missing document, etc.)

## üéØ Success Criteria

Phase 1 ho√†n th√†nh khi:
1. ‚úÖ Upload file th√†nh c√¥ng
2. ‚úÖ Document ƒë∆∞·ª£c process (extract ‚Üí chunk ‚Üí embed)
3. ‚úÖ Chat v·ªõi document tr·∫£ v·ªÅ response d·ª±a tr√™n content
4. ‚úÖ Vector search t√¨m ƒë∆∞·ª£c relevant chunks
5. ‚úÖ Streaming response ho·∫°t ƒë·ªông

Sau ƒë√≥ c√≥ th·ªÉ commit v√† push l√™n GitHub! üöÄ

