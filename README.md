# VeritasAI Python - RAG System

A Retrieval-Augmented Generation (RAG) system built with Django and Python, demonstrating how Python compares to PHP/Laravel for AI applications.

## ğŸ¯ What is RAG?

**RAG** (Retrieval-Augmented Generation) combines:
- **Retrieval**: Smart search to find relevant information from your documents
- **Generation**: LLM to generate natural answers based on retrieved context

**Why RAG?**
- âœ… AI answers based on **your private documents** (not just training data)
- âœ… Reduces hallucination (AI making things up)
- âœ… Update knowledge without retraining models
- âœ… Cite sources (know where information comes from)

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     RAG SYSTEM ARCHITECTURE                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   USER INTERFACE     â”‚         â”‚   ADMIN INTERFACE    â”‚
â”‚  (Chat, Documents)   â”‚         â”‚ (Upload, Manage)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                                â”‚
           â”‚                                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WEB APPLICATION                         â”‚
â”‚                  (Django - Views/URLs)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                        â”‚
       â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€ TWO MAIN FLOWS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
       â”‚ â”‚                                  â”‚  â”‚
       â”‚ â”‚  1ï¸âƒ£  INDEXING (Document Upload) â”‚  â”‚
       â”‚ â”‚  2ï¸âƒ£  QUERYING (Chat with AI)    â”‚  â”‚
       â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
       â”‚                                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BACKGROUND JOBS      â”‚           â”‚   SERVICES LAYER       â”‚
â”‚  (Celery/Subprocess)  â”‚           â”‚  (Business Logic)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA STORAGE                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ PostgreSQL  â”‚  â”‚   pgvector   â”‚  â”‚  File System â”‚     â”‚
â”‚  â”‚ (Metadata)  â”‚  â”‚  (Vectors)   â”‚  â”‚  (Documents) â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EXTERNAL SERVICES                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   Ollama    â”‚  â”‚    Redis     â”‚  â”‚    Celery    â”‚     â”‚
â”‚  â”‚(Embeddings, â”‚  â”‚   (Cache)    â”‚  â”‚   Workers    â”‚     â”‚
â”‚  â”‚    LLM)     â”‚  â”‚              â”‚  â”‚  (Optional)  â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¤ Flow 1: INDEXING (Document Upload & Processing)

**Purpose**: Convert documents into searchable vectors

```
USER
  â”‚
  â”‚ Upload PDF/DOCX/TXT
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. UPLOAD ENDPOINT                                      â”‚
â”‚    - Validate file type and size                        â”‚
â”‚    - Check duplicates (SHA-256 hash)                    â”‚
â”‚    - Save to file system                                â”‚
â”‚    - Create Document record (status: pending)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚ Trigger background job
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. BACKGROUND PROCESSING                                â”‚
â”‚    - Celery worker (if available)                       â”‚
â”‚    - OR subprocess: python manage.py process_document   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. TEXT EXTRACTION (TextExtractionService)             â”‚
â”‚    - PDF â†’ PyPDF2.PdfReader                            â”‚
â”‚    - DOCX â†’ python-docx                                 â”‚
â”‚    - TXT/MD â†’ Plain text                                â”‚
â”‚    â†’ Output: Raw text string                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. TEXT CHUNKING (RecursiveChunkingService)            â”‚
â”‚    - Split by semantic units: \n\n â†’ \n â†’ . â†’ space   â”‚
â”‚    - Chunk size: ~300 words (1500 chars)               â”‚
â”‚    - Overlap: 200 chars (13%) for context preservation â”‚
â”‚    â†’ Output: ["chunk1", "chunk2", ...]                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. EMBEDDING GENERATION (EmbeddingService)             â”‚
â”‚    - Batch process chunks (10 at a time)               â”‚
â”‚    - Call Ollama API: POST /api/embeddings             â”‚
â”‚    - Model: nomic-embed-text                            â”‚
â”‚    - Each chunk â†’ 768-dimensional vector                â”‚
â”‚    â†’ Output: [[0.123, -0.456, ...], [...]]            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. VECTOR STORAGE (PostgreSQL + pgvector)              â”‚
â”‚    - Store in DocumentChunk table:                      â”‚
â”‚      â€¢ content (text)                                   â”‚
â”‚      â€¢ embedding (vector(768))                          â”‚
â”‚      â€¢ token_count (int, pre-computed)                  â”‚
â”‚    - Create vector index for fast similarity search    â”‚
â”‚    - Update Document.status = 'completed'              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Result**: Document is now indexed and ready for semantic search!

## ğŸ’¬ Flow 2: QUERYING (Chat with AI)

**Purpose**: Answer questions based on indexed documents

```
USER
  â”‚
  â”‚ "What does the document say about Python?"
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. CHAT ENDPOINT                                        â”‚
â”‚    - Receive user question                              â”‚
â”‚    - Get document_id                                    â”‚
â”‚    - Load chat history (last 10 messages)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. QUERY EMBEDDING (EmbeddingService + Redis Cache)    â”‚
â”‚    - Check Redis cache first (key: hash(question))     â”‚
â”‚    - If miss: Generate embedding via Ollama            â”‚
â”‚    - Cache result for 1 hour                            â”‚
â”‚    â†’ Output: [0.234, -0.567, ...] (768-dim vector)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. VECTOR SIMILARITY SEARCH (pgvector)                 â”‚
â”‚    - SQL: SELECT * FROM chunks                          â”‚
â”‚           WHERE document_id = ?                         â”‚
â”‚           ORDER BY embedding <=> query_vector           â”‚
â”‚           LIMIT 10                                      â”‚
â”‚    - Operator <=> : Cosine distance                     â”‚
â”‚    â†’ Returns top 10 most similar chunks                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. CONTEXT BUILDING (RagPromptService)                 â”‚
â”‚    - Filter chunks by similarity threshold              â”‚
â”‚    - Sort by relevance score                            â”‚
â”‚    - Build context within token limit (~4000 tokens)   â”‚
â”‚    - Format prompt with context + question              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. LLM GENERATION (OllamaClient)                       â”‚
â”‚    - Call Ollama: POST /api/chat (streaming)           â”‚
â”‚    - Model: llama3.1                                    â”‚
â”‚    - Stream response token by token                     â”‚
â”‚    â†’ Output: "The document mentions Python..."         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. STREAMING RESPONSE                                   â”‚
â”‚    - Stream to frontend via Server-Sent Events (SSE)   â”‚
â”‚    - Save messages to DB (async, background thread)    â”‚
â”‚    - Display to user in real-time                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ RAG vs Traditional Search

### Traditional Search (Keyword-based):
```
Query: "Python programming"
Search: WHERE content LIKE '%Python%' AND '%programming%'
âŒ Problem: Only matches exact words, no semantic understanding
```

### RAG (Semantic Search):
```
Query: "NgÃ´n ngá»¯ láº­p trÃ¬nh Python" (Vietnamese)
Embedding: [0.23, -0.45, ...]
Search: Vector similarity (cosine distance)
âœ… Result: Finds chunks about Python even without exact words!
```

## ğŸ› ï¸ Tech Stack

### Backend
- **Framework**: Django 5.2.8
- **Language**: Python 3.13
- **Database**: PostgreSQL 16 with pgvector extension
- **Cache**: Redis 7+
- **Task Queue**: Celery 5.4 (optional, auto-fallback to subprocess)

### AI/ML
- **LLM**: Ollama (llama3.1 for chat)
- **Embeddings**: nomic-embed-text (768 dimensions)
- **Vector Search**: pgvector with cosine distance

### Frontend
- **Templates**: Django Templates
- **JavaScript**: Alpine.js 3.x
- **CSS**: Tailwind CSS 3.x
- **Icons**: Heroicons

### Services
- **Text Extraction**: PyPDF2, python-docx
- **Chunking**: Custom recursive splitter
- **Embeddings**: Batch processing with retry logic

## ğŸ“Š Key Features

### Performance Optimizations
1. **Database Connection Pooling** - Reuse connections (CONN_MAX_AGE)
2. **Redis Caching** - Cache query embeddings for 1 hour
3. **Pre-computed Token Counts** - Stored in DB, no recalculation
4. **N+1 Query Fix** - Batch fetch with proper ORM usage
5. **Async Message Saving** - Background threads for non-blocking writes
6. **Batch Embeddings** - Process 10 chunks at once

### Chunking Strategy
- **Chunk Size**: ~300 words (1500 characters)
- **Overlap**: 200 characters (13%)
- **Splitters**: Semantic units (paragraphs â†’ sentences â†’ words)
- **Optimal for**: 1K - 100K word documents

### Error Handling
- Auto-fallback from Celery to subprocess if workers unavailable
- Retry logic for embedding API calls (3 attempts)
- Clear error messages for different failure scenarios
- Graceful handling of scanned PDFs (image-based, needs OCR)

## ğŸ”§ Quick Start

### Prerequisites
```bash
# System requirements
Python 3.13+
PostgreSQL 16+ with pgvector
Redis 7+
Ollama (local LLM server)
```

### Installation
```bash
# Clone repository
git clone https://github.com/cam-hm/VeritasAI_Python.git
cd VeritasAI_Python

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Setup environment
cp .env.example .env
# Edit .env with your database credentials

# Run migrations
python manage.py migrate

# Create superuser (optional)
python manage.py createsuperuser

# Start services (see SETUP_SERVICES.md for details)
# 1. PostgreSQL with pgvector
# 2. Redis server
# 3. Ollama with models
ollama pull llama3.1
ollama pull nomic-embed-text

# Start Django server
python manage.py runserver

# (Optional) Start Celery worker
celery -A app.celery_app worker -l info
```

### Usage
1. Open http://127.0.0.1:8000
2. Upload a document (PDF, DOCX, TXT, MD)
3. Wait for processing to complete
4. Navigate to document detail page
5. Chat with AI about your document!

## ğŸ“ Project Structure

```
VeritasAI_Python/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ management/
â”‚   â”‚   â””â”€â”€ commands/
â”‚   â”‚       â””â”€â”€ process_document.py    # Django management command
â”‚   â”œâ”€â”€ models.py                      # Django ORM models
â”‚   â”œâ”€â”€ views.py                       # HTTP endpoints
â”‚   â”œâ”€â”€ urls.py                        # URL routing
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ text_extraction_service.py
â”‚   â”‚   â”œâ”€â”€ chunking_service.py
â”‚   â”‚   â”œâ”€â”€ embedding_service.py
â”‚   â”‚   â”œâ”€â”€ rag_prompt_service.py
â”‚   â”‚   â”œâ”€â”€ token_estimation_service.py
â”‚   â”‚   â””â”€â”€ ollama_client.py           # Ollama API wrapper
â”‚   â”œâ”€â”€ tasks/
â”‚   â”‚   â””â”€â”€ document_tasks.py          # Celery tasks
â”‚   â””â”€â”€ templates/                     # Django templates
â”œâ”€â”€ veritasai_django/
â”‚   â”œâ”€â”€ settings.py                    # Django settings
â”‚   â””â”€â”€ urls.py                        # Root URL config
â”œâ”€â”€ storage/
â”‚   â”œâ”€â”€ documents/                     # Uploaded files
â”‚   â””â”€â”€ logs/                          # Processing logs
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ manage.py                          # Django management script
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ TESTING.md                         # Testing guide
â””â”€â”€ SETUP_SERVICES.md                  # Services setup guide
```

## ğŸ”„ Django vs Laravel Comparison

| Component | Django (Python) | Laravel (PHP) |
|-----------|----------------|---------------|
| **Web Framework** | Django Views | Controllers |
| **ORM** | Django Models | Eloquent |
| **Background Jobs** | Celery + Redis | Laravel Queue |
| **Routing** | urls.py | routes/web.php |
| **Templates** | Django Templates | Blade |
| **Migrations** | Django Migrations | Laravel Migrations |
| **Admin Panel** | Django Admin | Laravel Nova |
| **Cache** | Redis/LocMem | Laravel Cache |
| **CLI** | manage.py | php artisan |
| **Package Manager** | pip | composer |

### Architectural Patterns
- **Django**: MTV (Model-Template-View) - variation of MVC
- **Laravel**: MVC (Model-View-Controller)

**Key Similarity**: Both provide full-stack web frameworks with built-in ORM, routing, templating, and admin interfaces.

## ğŸš€ Performance Metrics

### Document Processing
- **Small document** (1K words): ~5-10 seconds
- **Medium document** (10K words): ~30-60 seconds
- **Large document** (100K words): ~5-10 minutes

### Chat Response
- **Time to First Token (TTFT)**: ~500ms - 1s
- **Streaming Speed**: ~20-50 tokens/second (depends on Ollama model)

### Optimizations Impact
- **Redis cache hit**: 200ms saved per query
- **Pre-computed tokens**: 50ms saved per chunk
- **N+1 fix**: 80% reduction in DB queries
- **Connection pooling**: 30% reduction in query latency

## ğŸ§ª Testing

See [TESTING.md](TESTING.md) for detailed testing instructions.

```bash
# Run all tests
python manage.py test

# Test specific app
python manage.py test app

# Monitor upload status
./monitor_upload.sh

# Debug upload process
./debug_upload.sh
```

## ğŸ“š Documentation

- **[SETUP_SERVICES.md](SETUP_SERVICES.md)**: How to setup PostgreSQL, Redis, Ollama
- **[TESTING.md](TESTING.md)**: Testing guide and troubleshooting
- **[Django Documentation](https://docs.djangoproject.com/)**: Official Django docs
- **[pgvector](https://github.com/pgvector/pgvector)**: Vector similarity search
- **[Ollama](https://ollama.ai/)**: Local LLM server

## ğŸ¤ Contributing

This project is a learning exercise comparing Python/Django with PHP/Laravel for AI applications.

Feel free to:
- Report bugs
- Suggest improvements
- Submit pull requests

## ğŸ“ License

MIT License - see LICENSE file for details

## ğŸ‘¤ Author

**HoÃ ng Máº¡nh Cáº§m**
- GitHub: [@cam-hm](https://github.com/cam-hm)
- Project: Learning Python for AI applications

## ğŸ™ Acknowledgments

- Django team for the excellent web framework
- Ollama team for local LLM capabilities
- pgvector team for vector similarity search in PostgreSQL
- LangChain community for RAG patterns and best practices

---

**Note**: This is a learning project to compare Python's advantages over PHP for AI applications. The codebase includes detailed comments explaining Django concepts in relation to Laravel equivalents.
